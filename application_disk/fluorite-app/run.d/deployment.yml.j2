apiVersion: v1
kind: ConfigMap
metadata:
  name: caddy-global-options
  namespace: caddy-system
data:
  acmeCA: internal
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: fluorite
  annotations:
    kubernetes.io/ingress.class: caddy
spec:
  rules:
  - host: api.cloud.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: fluorite
            port:
              number: 80
---
kind: Service
apiVersion: v1
metadata:
  name: fluorite
spec:
  type: ClusterIP
  selector:
    app: fluorite
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 8000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluorite
  labels:
    app: fluorite
spec:
  selector:
    matchLabels:
      app: fluorite
  template:
    metadata:
      labels:
        app: fluorite
    spec:
      restartPolicy: Always
      volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 50Gi
      - name: model-store
        hostPath:
          path: /dev/shm/model/{{ model }}
      - name: model-engine
        hostPath:
          path: /engines/1-gpu
      - name: batcher
        hostPath:
          path: /inflight_batcher_llm
      - name: launch-script
        hostPath:
          path: /launch_script.sh
      - name: launch-server
        hostPath:
          path: /tensorrtllm_backend/scripts/launch_triton_server.py
      - name: backend
        hostPath:
          path: /tensorrtllm_backend
      runtimeClassName: nvidia
      containers:
        - name: fluorite
          image: nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3
          imagePullPolicy: Never
          command: ["bash"]
          args: ["/launch_script.sh"]
          ports:
          - containerPort: 8000
          volumeMounts:
          - mountPath: /dev/shm
            name: shm
          - name: model-store
            mountPath: /{{ model }}
          - name: model-engine
            mountPath: /engines/1-gpu
          - name: batcher
            mountPath: /inflight_batcher_llm
          - name: launch-script
            mountPath: /launch_script.sh
          - name: launch-server
            mountPath: /launch_triton_server.py
          - name: backend
            mountPath: /tensorrtllm_backend
          resources:
            limits: 
              nvidia.com/gpu: 1
          env:
          - name: NVIDIA_VISIBLE_DEVICES
            value: all
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: all
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: attestation-server
  namespace: caddy-system
  labels:
    app: attestation-server
spec:
  selector:
    matchLabels:
      app: attestation-server
  template:
    metadata:
      labels:
        app: attestation-server
    spec:
      restartPolicy: Always
      volumes:
        - name: attestation-store
          persistentVolumeClaim:
            claimName: local-path-pvc

      containers:
        - name: attestation-server
          image: docker.io/library/attestation-server:latest
          imagePullPolicy: Never
          env:
          - name: PORT
            value: "80"
          ports:
          - containerPort: 80
          volumeMounts:
          - name: attestation-store
            mountPath: /web
            readOnly: true
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: attestation-server
  namespace: caddy-system
  annotations:
    kubernetes.io/ingress.class: caddy
spec:
  rules:
  - host: attestation-endpoint.api.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: attestation-server
            port:
              number: 80
---
kind: Service
apiVersion: v1
metadata:
  name: attestation-server
  namespace: caddy-system
spec:
  type: ClusterIP
  selector:
    app: attestation-server
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: tpm-device-plugin
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: tpm-device-plugin
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        name: tpm-device-plugin
    spec:
      priorityClassName: "system-node-critical"
      containers:
      - image: k8s-tpm-device-plugin:latest
        imagePullPolicy: Never
        name: tpm-device-plugin
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - name: device-plugins
          mountPath: /var/lib/kubelet/device-plugins
      volumes:
      - name: device-plugins
        hostPath:
          path: /var/lib/kubelet/device-plugins
---
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      # This annotation is deprecated. Kept here for backward compatibility
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      # This toleration is deprecated. Kept here for backward compatibility
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      # Mark this pod as a critical add-on; when enabled, the critical add-on
      # scheduler reserves resources for critical add-on pods so that they can
      # be rescheduled after a failure.
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      priorityClassName: "system-node-critical"
      runtimeClassName: nvidia
      containers:
      - image: docker.io/library/nvidia-device-plugin:latest
        imagePullPolicy: Never
        name: nvidia-device-plugin-ctr
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-caddy-ns
  namespace: caddy-system
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: caddy-ingress
  namespace: caddy-system
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: caddy-ingress-controller
  ingress:
  - from:
    - ipBlock:
        cidr: 0.0.0.0/0
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: caddy-egress
  namespace: caddy-system
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: caddy-ingress-controller
  policyTypes:
  - Egress
  egress:
  - {}
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-attestation-server
  namespace: caddy-system
spec:
  podSelector:
    matchLabels:
      app: attestation-server
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: "caddy-system"
      podSelector:
        matchLabels:
          app.kubernetes.io/name: caddy-ingress-controller
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-fluorite
spec:
  podSelector:
    matchLabels:
      app: fluorite
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: "caddy-system"
      podSelector:
        matchLabels:
          app.kubernetes.io/name: caddy-ingress-controller